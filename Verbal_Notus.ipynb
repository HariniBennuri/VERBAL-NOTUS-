{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTFG7duroq99",
        "outputId": "59b7688a-472a-4e85-8e1a-d8d28182f21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO73o3yCpKi8"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEcjYs3Nq4RJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7HnVbh9rtPA"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/datasets/awsaf49/ljspeech-sr16k-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knfx3QKBtAC0"
      },
      "outputs": [],
      "source": [
        "#https://keithito.com/LJ-Speech-Dataset/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Now defining a simple deep learning model for ASR\n",
        "class ASRModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ASRModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Defining the vocabulary (characters) based on the dataset's transcriptions\n",
        "\n",
        "vocabulary = set()\n",
        "for transcription in lj_dataset.metadata['Normalized Transcription']:\n",
        "    vocabulary.update(transcription)\n",
        "\n",
        "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
        "vocabulary.update(special_tokens)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(sorted(vocabulary))}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "output_size = len(vocabulary)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 161\n",
        "hidden_size = 256\n",
        "output_size = len(alphabet)\n",
        "\n",
        "# Initializing the ASR model\n",
        "model = ASRModel(input_size, hidden_size, output_size)\n",
        "\n",
        "# Defining the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Defining a function to load and preprocess the audio data\n",
        "def load_and_preprocess_audio(audio_path, target_sample_rate=22050):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if sample_rate != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(\n",
        "            orig_freq=sample_rate, new_freq=target_sample_rate\n",
        "        )\n",
        "        waveform = resampler(waveform)\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=target_sample_rate, n_mfcc=13\n",
        "    )\n",
        "    mfcc = mfcc_transform(waveform)\n",
        "    return mfcc\n",
        "\n",
        "# Creating a custom dataset class\n",
        "class LJSpeechDataset(Dataset):\n",
        "    def __init__(self, metadata_file, audio_dir, target_sample_rate=22050):\n",
        "        self.metadata = pd.read_csv(metadata_file, delimiter='|')\n",
        "        self.audio_dir = audio_dir\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_filename = os.path.join(self.audio_dir, self.metadata['ID'][idx])\n",
        "        transcription = self.metadata['Normalized Transcription'][idx]\n",
        "\n",
        "        # Loading and preprocessing the audio\n",
        "        audio_data = load_and_preprocess_audio(audio_filename, self.target_sample_rate)\n",
        "\n",
        "        return audio_data, transcription\n",
        "\n",
        "metadata_file = '/content/drive/MyDrive/metadata.csv'\n",
        "audio_dir = '/content/drive/MyDrive/audio/files'\n",
        "\n",
        "# Creating an instance of the LJSpeechDataset\n",
        "lj_dataset = LJSpeechDataset(metadata_file, audio_dir)\n",
        "\n",
        "# Accessing data and transcriptions using dataset[index]\n",
        "sample_audio, transcription = lj_dataset[0]\n",
        "print(f\"Transcription: {transcription}\")\n",
        "print(f\"Audio Shape: {sample_audio.shape}\")\n",
        "\n",
        "# Creating a custom dataset class\n",
        "class SpeechRecognitionDataset(Dataset):\n",
        "    def __init__(self, metadata_file, transform=None):\n",
        "        self.data, self.labels = load_dataset(metadata_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'audio': self.data[idx], 'label': self.labels[idx]}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "metadata_file = 'path_to_metadata_file.csv'  # Replace with the actual path\n",
        "train_dataset, val_dataset = train_test_split(metadata_file, test_size=0.2)\n",
        "\n",
        "# Creating data loaders for training and validation\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        inputs, labels = batch['audio'], batch['label']\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, output_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Validating loop (evaluate the model)\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "        inputs, labels = batch['audio'], batch['label']\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 2)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0) * labels.size(1)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uON-QOeYTXcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnQ_jeJLuga9"
      },
      "outputs": [],
      "source": [
        "data_url=\"https://www.kaggle.com/datasets/awsaf49/ljspeech-sr16k-dataset\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E8l4UZStH43"
      },
      "outputs": [],
      "source": [
        "wavs_path = data_path + \"/wavs/\"\n",
        "metadata_path = data_path+\"/metadata.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux6ZOMWsuclL"
      },
      "outputs": [],
      "source": [
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCzjrKXmufOc"
      },
      "outputs": [],
      "source": [
        "metadata_df.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFKZTDG8wIgq"
      },
      "outputs": [],
      "source": [
        "metadata_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsWvdnWKw88x"
      },
      "outputs": [],
      "source": [
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "metadata_df.head (3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbLUVBVVxQc5"
      },
      "outputs": [],
      "source": [
        "#splitl=int(len(metadata_df) * 0.30)\n",
        "split = int(len(metadata_df) * 0.90)\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the training set: {len(df_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae32rJnmy4Mg"
      },
      "outputs": [],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras. layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)\n",
        "print(\n",
        "f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRJ46WiQ0LZm"
      },
      "outputs": [],
      "source": [
        "frame_length = 256\n",
        "frame_step= 160\n",
        "fft_length = 384\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    audio, _= tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    audio = tf.cast (audio, tf.float32)\n",
        "    spectrogram = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram=(spectrogram I means) / (stddevs + 1e-10)\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split (label, input_encoding=\"UTF-8\")\n",
        "    label = char_to_num(label)\n",
        "    return spectrogram, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFCHQ75T5TkD"
      },
      "source": [
        "#Creating data set object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keWzofH_5Jjp"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices ((list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"])))\n",
        "train_dataset = (\n",
        "  train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        " .padded batch (batch_size)\n",
        " .prefetch(buffer_size=tf.data.AUTOTUNE) I\n",
        ")\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices (\n",
        "     (list(df_val[\"file_name\"]), list (df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "  validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        " .padded_batch (batch_size)\n",
        " .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1m5Fv3D66Om"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    #Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)). numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    #Wav\n",
        "    file= tf.io.read_file(wavs_path + list(df train[\"file_name\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlin(0, len(audio))\n",
        "    display.display(display.Audio (np. transpose (audio), rate=16000))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G93OBHi8WBa"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOR44IHS8l7e"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "  batch_len = tf.cast(tf.shape(y_true) [0], dtype=\"int64\")\n",
        "  input_length= tf.cast(tf.shape (y_pred) [1], dtype=\"int64\")\n",
        "  label_length = tf.cast(tf.shape (y_true) [1], dtype=\"int64\")\n",
        "\n",
        "  input_length = input_length * tf.ones(shape=(batch_len,1), dtype=\"int64\")\n",
        "  label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "  loss = keras.backend.ctc_batch_cost (y_true, y_pred, input_length, label_length)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu6Qvs1leBN5"
      },
      "source": [
        "#Model DeepSpeech2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NYx5aQW9jXp"
      },
      "outputs": [],
      "source": [
        "def build_model(input_dim, output_dim, ran_layers=5, rnn_units=128):\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\") (input_spectrogram)\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\") (x)\n",
        "    x = layers.ReLU(name=\"cony_1_relu\") (x)\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers. ReLU(name=\"conv 2 relu\")(x)\n",
        "    x = layers.Reshape((-1, x.shape[-2] x.shape[-1]))(x)\n",
        "\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent layers.GRU(\n",
        "            units = rnn_units,\n",
        "            activations=\" tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use=bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x= layers.Bidirectional(\n",
        "           recurrent, name=f\"bidirectional _{i}\", merge_mode=\"concat\")(x)\n",
        "        if i  <  rnn_layers:\n",
        "          x= layers.Dropout (rate=0.5)(x)\n",
        "\n",
        "    x = layers.Dense (units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x= layers.ReLU(name=\"dense_1_relu\") (x)\n",
        "    x = layers.Dropout (rate=8.5)(x)\n",
        "\n",
        "    output = layers.Dense (units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    opt =  keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    input dim=fft_length // 2+1,\n",
        "    output_dim=char_to_num. vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_m5AdW-GEir"
      },
      "source": [
        "#Training and Evaluating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZWp0NQyJe1Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings. reduce_join (num_to_char(result)). numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    def  __init__(self, dataset):\n",
        "         super()._init_()\n",
        "         self.dataset = dataset\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets =[]\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions=model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions (batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (tf.strings.reduce_join(num_to_char(label)). numpy().decode(\"utf-8\"))\n",
        "                targets.append(label)\n",
        "    wer_score = wer(targets, predictions)\n",
        "    print(\"-\" * 100)\n",
        "    print (f\"Word Error Rate: {wer_score: 4f}\")\n",
        "    print(\"- \" * 100)\n",
        "    for i in np.random.randint(0, len(predictions), 2):\n",
        "        print(f\"Target        : {targets[i]}\")\n",
        "        print(f\"Prediction: {predictions[i]}\")\n",
        "        print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRqoy3K-ORaF"
      },
      "source": [
        "let start the trainning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skrEugX9Jf1I"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "history = model.fit(\n",
        "train_dataset,\n",
        "validation_data=validation_ dataset,\n",
        "epochs = epochs,\n",
        "callbacks=[validation_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RyN2fRNOsAI"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt7Ef4zNJfrz"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions= model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score: 4f}\")\n",
        "print(\"-\" * 180)\n",
        "for i in np.random.randint(s, len(predictions), 5):\n",
        "    print(f\"Target (targets[i]}\")\n",
        "    print(f\"Prediction: (predictions[1]}\")\n",
        "    print(\"-\", * 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}